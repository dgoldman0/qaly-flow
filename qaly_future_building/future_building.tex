\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{orcidlink}
\usepackage[numbers]{natbib}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\title{Future-Building as Maintenance:\\Rate-Based Health Evaluation for Indefinite Horizons}
\author{Daniel S. Goldman \orcidlink{0000-0003-2835-3521}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Standard health evaluation sums quality-adjusted life years over a lifetime, but that sum becomes meaningless when biological lifespan has no fixed upper bound. A shift from sum to rate---measuring sustained quality of life per unit time---resolves the problem and, more interestingly, reveals deep connections between aging biology, existential-risk governance, and institutional measurement design. The hallmarks-of-aging framework already describes health in the language of failure modes and maintenance; reliability engineering has long applied similar reasoning to complex systems; and the futurist concern with keeping the long-run future accessible turns out to be the same problem viewed at civilisational scale. The central challenge is governance: any rate worth optimising will be gamed, and the measurement system that supports it needs the same ongoing, adaptive maintenance as the bodies and institutions it tracks. The treatment remains agnostic about mathematical formalisation so that the argument survives any future rework of the underlying formalism.
\end{abstract}

\section{A world without a finish line}

For most of recorded history, the question ``how long will this person live?'' had a rough but serviceable answer: not very long, by modern standards, and with an upper bound that everyone understood. Public health could plan around that bound. Insurance could price it. Governments could discount future costs against it. The entire apparatus of health evaluation---cost-effectiveness analysis, technology assessment, resource allocation---grew up inside a world where lives had a characteristic shape: they started, they lasted a few decades, and they ended.

That shape is starting to change. Simon Melov's review of the geroscience programme describes a research agenda organised around a single shift in target: rather than treating chronic diseases one at a time, go after the aging processes that produce them all \citep{melov2016geroscience}. If that programme succeeds---even partially---then the planning horizon for a human life stretches in ways that quietly break the standard evaluation toolkit.

Quality-adjusted life years, the workhorse metric of health technology assessment, are defined as a sum. You take the quality of each year lived, you add them up, and you compare interventions by how many QALYs they buy. That sum presupposes a terminus. When the horizon is open-ended, the sum either diverges or depends on an arbitrary cutoff, and cost-effectiveness ratios built on it inherit the same fragility.

The fix is to measure a rate instead: quality of life per unit time, sustained over an ongoing regime. The long-run average reward per unit time in a regenerative process has clean existence conditions in renewal-reward theory \citep{sigman2018renewalreward, gallager2011renewalprocesses} and extends naturally to controlled settings through average-cost optimisation \citep{feinberg2012averagecostmdp}. The mathematics matters less here than the conceptual shift. A rate stays bounded and interpretable whether the horizon is ten years or ten thousand. It captures how well life is going, how much effort maintenance requires, and how often catastrophe interrupts---in one object.

Call that object QALY flow.

\subsection*{Ensemble Context}

\textbf{Paper ID:} \texttt{d7006442-c67e}

This paper is part of a four-paper ensemble on health evaluation under indefinite horizons. The root paper (\texttt{c2f3de53-f086}) defines the core mathematical framework---QALY flow as a renewal-reward ratio, with formal results on existence, cost-effectiveness, perturbation stability, and distributional aggregation. This paper connects aging biology, reliability engineering, existential-risk governance, and institutional measurement design under a unified maintenance framing, showing how the rate-based perspective operates across scales without requiring the formal apparatus. Companion papers explore the civilizational context of longevity (\texttt{73997063-e1ed}) and translate the framework into public health practice (\texttt{80fbe599-356c}).

\section{Aging as engineering}

In 2013, Carlos López-Otín and colleagues published a taxonomy of aging that reads, almost eerily, like a failure-mode-and-effects analysis for a biological system \citep{lopezotin2013hallmarks}. Genomic instability, telomere attrition, epigenetic drift, loss of proteostasis, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, deregulated nutrient sensing, altered intercellular communication---each hallmark identifies a subsystem whose gradual degradation contributes to the organism's declining performance. Their 2023 update expanded the list to include chronic inflammation, dysbiosis, and a web of cross-hallmark interactions, making the picture look less like a checklist and more like a complex system with coupled failure modes \citep{lopezotin2023expandinghallmarks}.

Maxim Finkelstein had already made the engineering parallel explicit. His work at the Max Planck Institute mapped reliability-theory concepts---redundancy depletion, damage accumulation, hazard-rate dynamics---onto biological aging trajectories \citep{finkelstein2006engineeringreliability}. The conceptual move is to treat mortality and morbidity as the output of a system whose components degrade, whose redundancy erodes, and whose performance can be modelled with the same tools that aerospace and energy engineers use for aircraft engines and power grids.

Once that move is made, geroscience starts to look like a maintenance engineering programme. Instead of waiting for a specific subsystem to fail and then attempting a targeted repair---treating heart disease, excising a tumour---you invest in the conditions that keep multiple subsystems operating within tolerance. Prevention becomes quality assurance. Monitoring becomes condition-based maintenance. And the success criterion shifts from ``did we fix the problem?'' to ``is the system still running well?''---which is exactly what a rate measures.

The analogy is imperfect in one important respect, and the imperfection is itself instructive. In industrial maintenance, Alberto Pliego Marugán and colleagues have studied what happens when repairs are \emph{increasingly imperfect}: each intervention restores less function than the last, and the optimal policy has to adapt, shifting from repair toward replacement as the system ages \citep{marugan2024rlmaintenance}. Biology almost certainly behaves this way. The first round of a senolytic therapy may clear senescent cells effectively; the tenth round operates on a body that has accumulated other forms of damage, in tissues that have been repeatedly stressed, with compensatory mechanisms that may have shifted. The engineering literature's response to imperfect repair is adaptive control---adjust the maintenance schedule, change the intervention mix, re-evaluate the monitoring thresholds---and that response translates directly into a design principle for health systems under indefinite horizons: the maintenance policy itself must be maintained.

Here is where the rate perspective starts to earn its keep. Under a sum-based evaluation, an intervention that adds five healthy years scores well regardless of what happens afterwards. Under a rate-based evaluation, an intervention is valuable only insofar as it contributes to sustained performance. A therapy that produces a burst of improvement followed by accelerated decline looks good in the sum and bad in the rate. A preventive programme that slightly reduces the pace of degradation across decades looks modest in the sum and transformative in the rate. The metric changes what you value, and what you value changes what you build.


\section{Maintenance all the way up}

The engineering framing is compelling for bodies. It becomes more compelling---and more consequential---when you notice that the same framing applies at every scale above the individual.

Eileen Crimmins has argued for a set of ``social hallmarks of aging'' that parallel the biological ones \citep{crimmins2020socialhallmarks}. Education, income inequality, chronic stress exposure, environmental toxin burden, social isolation---these upstream conditions shape biological aging trajectories as powerfully as any cellular mechanism. People in deprived neighbourhoods age faster, measured biologically, than people in affluent ones. The hallmarks of aging are real, but they do not operate in a social vacuum; they are \emph{modulated} by the conditions in which people live.

Terrie Moffitt's work on the geroscience translation agenda sharpens the point \citep{moffitt2020behavioralsocial}. Even when a biological intervention works in the lab, its population-level impact depends on adherence, access, behaviour, and the social infrastructure that delivers it. The bottleneck between a promising therapy and a sustained improvement in population health is almost never the biology alone. It is the system of behaviour, institutions, and incentives through which the biology is deployed.

From the standpoint of QALY flow, this is a single observation with a long reach: the sustained rate of health quality is a property of the entire sociotechnical system, not of the biomedical subsystem alone. Reducing chronic stress is upstream maintenance on the biological hallmarks. Educational investment that improves health literacy is condition monitoring at scale. Limiting toxin exposure is preventive maintenance on the population's genome. Each of these interventions changes the steady-state distribution of health states across the population, and a rate-based metric registers that change.

The same logic extends to catastrophic risk. Nick Bostrom's work on existential risk argues that preserving the long-run future is extraordinarily valuable, because the potential future is vast and the loss from premature truncation is correspondingly enormous \citep{bostrom2013existentialriskprevention}. Hilary Greaves and William MacAskill formalise conditions under which the expected value of the far future dominates present-day considerations \citep{greaves2021stronglongtermism}. These arguments translate cleanly into rate language: catastrophic events reduce QALY flow both by destroying lives directly and by disrupting the maintenance systems that sustain quality for everyone else.

Bostrom's earlier paper on ``astronomical waste'' identified a striking asymmetry between speed and safety in technological development \citep{bostrom2003astronomicalwaste}. Accelerating progress increases the rate of value creation, but increasing catastrophic risk decreases the probability that the value-creating regime persists at all. Under a rate-based objective, that asymmetry becomes a portfolio constraint: you optimise the sustained rate subject to the requirement that the regime survives. Safety investment is infrastructure for the rate, the same way that pandemic preparedness is infrastructure for population health and building inspection is infrastructure for a city's housing stock.

The World Health Organization's framework for climate-resilient health systems illustrates what this looks like institutionally \citep{who2015climateresilient}. The framework organises adaptation around anticipation, response capacity, and recovery---exactly the dimensions that determine how a shock affects the sustained rate. A health system that can absorb a pandemic or a climate disaster and continue delivering care maintains QALY flow through the disruption. One that collapses under stress creates a period of severely reduced quality that drags the long-run average down, potentially for years after the acute event has passed.

So the maintenance picture scales: bodies are maintained by biological repair, social conditions, and behavioural support; populations are maintained by health systems, social infrastructure, and environmental standards; and the civilisational trajectory is maintained by catastrophe prevention, resilience engineering, and institutional coordination. QALY flow is legible at each of these levels because it measures the same thing at each level: how well the system is performing per unit time, on average, over the long run.


\section{Managing the measurement}

All of this---the maintenance engineering, the social determinants, the civilisational portfolio---assumes that you can actually measure QALY flow well enough to govern with it. The assumption deserves scrutiny, because the history of institutional metrics is a history of measurement corrupting itself.

Michael Stumborg and colleagues at CNA produced an unusually thorough catalogue of the ways that metrics degrade when they become targets \citep{stumborg2022goodharts}. The mechanisms are varied---proxy manipulation, strategic data omission, threshold gaming, reallocation of effort from unmeasured to measured activities---but the pattern is monotonous. Once a number matters for resource allocation, the people being measured find ways to improve the number that do not improve the thing the number was supposed to track. Hospital readmission rates go down because hospitals reclassify admissions. Test scores go up because teachers teach to the test. Crime statistics improve because police reclassify offences.

A QALY-flow metric deployed at scale would face every one of these pressures, intensified by the fact that it is designed for continuous optimisation rather than periodic reporting. A hospital system evaluated on sustained health quality per patient-year has incentives to select healthier patients, to classify complications as new episodes rather than ongoing ones, and to invest in the measurement apparatus---self-reported quality surveys, biomarker panels---in ways that inflate the reported rate without improving the underlying reality.

This is a genuine problem, and the shape of the solution is by now a familiar one. Measurement integrity is a maintenance problem. The measurement system degrades under incentive pressure the way a biological system degrades under oxidative stress or a building degrades under weather. It requires monitoring, repair, adaptation, and occasional replacement of components that have become too corrupted to salvage.

Holger Strulik's survey of aging measurement illustrates the challenge at the level of individual assessment: different constructions of ``biological age'' yield different rankings of individuals and different conclusions about intervention efficacy \citep{strulik2021measuringageing}. At the population level, the problem multiplies. Which quality-of-life instrument do you use? How do you handle non-response? How do you distinguish genuine improvement from reporting bias? Each of these is a maintenance task for the measurement system, and each requires ongoing attention.

The institutional precedents are instructive. The UK Treasury's Green Book codifies an appraisal process built around explicit options analysis, structured uncertainty treatment, and mandatory post-implementation review \citep{hmtreasury2026greenbook}. NIST's risk assessment framework organises ongoing evaluation around threat identification, vulnerability analysis, and iterative updating \citep{nist2012sp80030}. Both frameworks treat evaluation as a \emph{process} rather than an event---something that is never finished, only current. Governing a QALY-flow metric requires the same stance: the measurement is never ``done.'' It is an ongoing regime of data collection, integrity checking, adversarial auditing, and institutional accountability that must itself be evaluated and maintained.

The distributional dimension adds another layer. When you optimise a single aggregate rate, you naturally concentrate effort where improvements are cheapest. The people who are hardest to help---those with complex needs, poor baseline health, or limited access---become drags on the metric, and a system under performance pressure may quietly abandon them. Miqdad Asaria, Susan Griffin, and Richard Cookson developed distributional cost-effectiveness analysis specifically to make these dynamics visible \citep{asaria2013distributionalcea}. The technique defines outcomes for identifiable subgroups and applies explicit equity weights, so that the question ``who benefits?'' is answered alongside ``how much benefit?'' Under a QALY-flow regime, the distributional analysis operates on group-level rates rather than group-level sums, but the logic is identical: aggregate optimisation without distributional scrutiny produces aggregate numbers that mask concentrated harm.

Conventional tools do not vanish in this picture. The Green Book's discounting guidance remains entirely appropriate for bounded subproblems---the construction costs of a new hospital, the ramp-up phase of a new intervention, the payback period of a capital investment \citep{hmtreasury2026discounting}. Discounting and rate-based evaluation are complements: discounting handles the transitional and time-limited components of a policy, while QALY flow handles the steady-state, continuing-effects dimension. The Green Book already distinguishes between these categories; the rate metric fills a slot that the existing framework recognises but cannot currently populate under open-ended horizons.


\section{What remains}

The gaps are honest ones. Measuring a population's QALY flow under conditions where the measurement itself shapes behaviour---where monitoring changes adherence, where reporting incentives distort data, where the interventions being evaluated alter the social fabric that generates the data---is a problem that adaptive control theory can frame \citep{tadepalli1998average, wang2023robustaverage} but that institutional practice has barely begun to address. Setting safety margins for catastrophic risk when the models themselves may be misspecified is a hard problem in robust optimisation \citep{wang2023robustaverage, boucherie2023averagecostnotes}, and the gap between the theoretical tools and the practical governance of real institutions is wide. Designing portfolio governance that bridges short-horizon budgets with indefinite-horizon objectives requires integrating discounting practice \citep{hmtreasury2026discounting} with rate-based steady-state reasoning in ways that have no established template. And sustaining distributional fairness over horizons long enough for political coalitions to dissolve, reform, and dissolve again is a governance challenge that exceeds anything in the current literature on equity weighting \citep{asaria2013distributionalcea, crimmins2020socialhallmarks}.

Building the future, it turns out, is a maintenance job. And maintenance, done well, is what makes everything else possible. A person whose biology is sustained rather than deteriorating has time to master difficult skills, to undertake projects that span decades, to accumulate and transmit knowledge at a depth that a seventy-year window barely permits. A civilisation that has learned to maintain its population's health, its institutional integrity, and its capacity to absorb shocks has freed up the energy and attention that currently go to managing decline. The headaches of measurement, fairness, adaptive governance, and catastrophe prevention are real, and they are the price of entry to a world where human ambition is no longer truncated by human fragility.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
